# ğŸš¨ ç´§æ€¥ï¼šæ­£ç¡®é‡å¯ Worker

## é—®é¢˜åŸå› 

**å‘ç°æœ‰ 4 ä¸ªæ—§çš„ celery worker è¿›ç¨‹åŒæ—¶åœ¨è¿è¡Œï¼**

```
PID 37902 (8:10PM)
PID 31285 (7:29PM)  â† æ—§è¿›ç¨‹
PID 29428 (7:18PM)  â† æ—§è¿›ç¨‹
PID 37907 (8:10PM)
```

è¿™äº›æ—§è¿›ç¨‹ä»åœ¨ä½¿ç”¨**æ—§ä»£ç **ï¼ˆåŒ…å«åµŒå¥—å­—å…¸çš„ EXAMPLESï¼‰ï¼Œæ‰€ä»¥é”™è¯¯ç»§ç»­å‘ç”Ÿã€‚

---

## âœ… è§£å†³æ–¹æ¡ˆï¼šå½»åº•æ¸…ç†å¹¶é‡å¯

### æ–¹æ³• 1ï¼šä½¿ç”¨è‡ªåŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /Users/zhangrui/Desktop/konwledge-graph/GP_back_end_py
./restart_worker.sh
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. æ€æ­»æ‰€æœ‰æ—§çš„ celery è¿›ç¨‹
2. æ¸…ç† Python ç¼“å­˜
3. éªŒè¯ä»£ç æ›´æ–°
4. å¯åŠ¨æ–°çš„ worker

### æ–¹æ³• 2ï¼šæ‰‹åŠ¨æ‰§è¡Œ

```bash
# 1. å¼ºåˆ¶æ€æ­»æ‰€æœ‰ celery è¿›ç¨‹
pkill -9 -f "celery.*worker"

# 2. ç­‰å¾…ç¡®è®¤
sleep 3
ps aux | grep celery | grep -v grep
# åº”è¯¥æ²¡æœ‰è¾“å‡º

# 3. æ¸…ç† Python ç¼“å­˜
cd /Users/zhangrui/Desktop/konwledge-graph/GP_back_end_py
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null
find . -type f -name "*.pyc" -delete 2>/dev/null

# 4. é‡æ–°å¯åŠ¨ï¼ˆå‰å°è¿è¡Œï¼‰
celery -A app.services.crawl_tasks worker --loglevel=info --concurrency=1
```

---

## ğŸ” éªŒè¯ Worker å·²æ›´æ–°

### å¯åŠ¨åçœ‹åˆ°çš„æ—¥å¿—åº”è¯¥åŒ…å«ï¼š

```
[INFO] langextract extract params: model_id=gpt-4o-mini, ...
```

è¿™æ˜¯æ–°å¢çš„è°ƒè¯•æ—¥å¿—ï¼Œå¦‚æœçœ‹åˆ°è¯´æ˜ä»£ç å·²æ›´æ–°ã€‚

### æäº¤ä»»åŠ¡åï¼š

**âœ… æˆåŠŸæ ‡å¿—**ï¼š
```
[INFO] langextract extract success: total_extractions=X
[INFO] langextract response task_id=59 entities=N relations=M
```

**âŒ å¦‚æœä»ç„¶å¤±è´¥**ï¼Œä¼šçœ‹åˆ°æ›´è¯¦ç»†çš„æ—¥å¿—ï¼š
```
[WARNING] Unexpected extraction_text type: type=dict value={...}
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¼šæœ‰å¤šä¸ª worker è¿›ç¨‹ï¼Ÿ

**åŸå› **ï¼š
- å¤šæ¬¡è¿è¡Œ `celery worker` å‘½ä»¤
- æ²¡æœ‰æ­£ç¡®åœæ­¢æ—§è¿›ç¨‹
- ä½¿ç”¨ `&` åå°è¿è¡Œä½†æ²¡æœ‰è®°å½• PID

**è§£å†³**ï¼š
- ä½¿ç”¨ `pkill -9` å¼ºåˆ¶æ€æ­»æ‰€æœ‰
- åªå¯åŠ¨ä¸€ä¸ªæ–°çš„ worker
- æˆ–è€…ä½¿ç”¨è¿›ç¨‹ç®¡ç†å·¥å…·ï¼ˆå¦‚ supervisorï¼‰

### Q2: å¦‚ä½•ç¡®è®¤åªæœ‰ä¸€ä¸ª workerï¼Ÿ

```bash
ps aux | grep celery | grep -v grep | wc -l
```

åº”è¯¥åªè¾“å‡º `1`ï¼ˆæˆ– `2`ï¼Œå¦‚æœæœ‰å­è¿›ç¨‹ï¼‰ã€‚

### Q3: Worker å¯åŠ¨åç«‹å³é€€å‡ºï¼Ÿ

**æ£€æŸ¥**ï¼š
1. æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸
2. Redis è¿æ¥æ˜¯å¦æ­£å¸¸
3. é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®

```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
celery -A app.services.crawl_tasks worker --loglevel=debug
```

---

## ğŸ“‹ å®Œæ•´æ£€æŸ¥æ¸…å•

åœ¨é‡å¯åï¼ŒæŒ‰é¡ºåºæ£€æŸ¥ï¼š

- [ ] 1. æ‰€æœ‰æ—§è¿›ç¨‹å·²åœæ­¢
  ```bash
  ps aux | grep celery | grep -v grep
  # åº”è¯¥æ²¡æœ‰è¾“å‡º
  ```

- [ ] 2. Python ç¼“å­˜å·²æ¸…ç†
  ```bash
  find . -type d -name "__pycache__" | wc -l
  # åº”è¯¥æ˜¯ 0
  ```

- [ ] 3. ä»£ç å·²æ›´æ–°
  ```bash
  grep '"country":' app/services/langextract_client.py
  # åº”è¯¥çœ‹åˆ° "country": "USA",ï¼ˆå¹³é“ºï¼‰
  ```

- [ ] 4. Worker å·²å¯åŠ¨
  ```bash
  ps aux | grep celery | grep -v grep | wc -l
  # åº”è¯¥æ˜¯ 1 æˆ– 2
  ```

- [ ] 5. çœ‹åˆ°æ–°çš„è°ƒè¯•æ—¥å¿—
  ```
  [INFO] langextract extract params: ...
  ```

- [ ] 6. æ²¡æœ‰ç±»å‹é”™è¯¯
  ```
  # ä¸åº”è¯¥çœ‹åˆ°ï¼š
  [ERROR] Extraction text must be a string...
  ```

---

## ğŸ¯ ç«‹å³æ‰§è¡Œ

### ä¸€é”®é‡å¯

```bash
cd /Users/zhangrui/Desktop/konwledge-graph/GP_back_end_py && ./restart_worker.sh
```

### æˆ–è€…æ‰‹åŠ¨æ‰§è¡Œ

```bash
# æ€æ­»æ—§è¿›ç¨‹
pkill -9 -f "celery.*worker"

# ç­‰å¾… 3 ç§’
sleep 3

# æ¸…ç†ç¼“å­˜
cd /Users/zhangrui/Desktop/konwledge-graph/GP_back_end_py
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null

# å¯åŠ¨æ–° worker
celery -A app.services.crawl_tasks worker --loglevel=info --concurrency=1
```

---

## ğŸ“ å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨

### æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **ç¡®è®¤è¿›ç¨‹æ•°é‡**
   ```bash
   ps aux | grep celery | grep -v grep
   ```

2. **ç¡®è®¤ä»£ç ç‰ˆæœ¬**
   ```bash
   grep -A 5 '"name": "Acme Bio Inc."' app/services/langextract_client.py
   ```

3. **æœ€æ–°çš„ 50 è¡Œæ—¥å¿—**
   ```bash
   tail -50 worker.log
   ```

4. **Worker å¯åŠ¨å‘½ä»¤**
   - ä½ ç”¨çš„æ˜¯å“ªä¸ªå‘½ä»¤ï¼Ÿ
   - `celery -A app.services.crawl_tasks` è¿˜æ˜¯ `celery -A app.main.celery_app`ï¼Ÿ

---

## ğŸ‰ æˆåŠŸæ ‡å¿—

å¯åŠ¨åæäº¤ä»»åŠ¡ï¼Œåº”è¯¥çœ‹åˆ°ï¼š

```
[INFO] langextract extract params: model_id=gpt-4o-mini, extraction_passes=2, max_workers=1
[INFO] Starting sequential extraction passes for improved recall with 2 passes
[INFO] Starting extraction pass 1 of 2
[INFO] HTTP Request: POST https://yinli.one/v1/chat/completions "HTTP/1.1 200 OK"
[INFO] langextract extract success: total_extractions=15
[INFO] langextract response task_id=59 url=https://... entities=10 relations=5
```

**å…³é”®**ï¼šä¸å†çœ‹åˆ° `Extraction text must be a string` é”™è¯¯ï¼

---

**ğŸš€ ç°åœ¨å°±æ‰§è¡Œ `./restart_worker.sh`ï¼**

